{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f984b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700f3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2024 = pd.read_csv(\"../scraper/kdd2024/kdd2024_subsessions.csv\")\n",
    "df_2025 = pd.read_csv(\"../scraper/kdd2025/kdd2025_subsessions.csv\")\n",
    "df_2024.shape, df_2025.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408b565",
   "metadata": {},
   "outputs": [],
   "source": [
    "titles_2024 = df_2024[\"title\"].tolist()\n",
    "titles_2025 = df_2025[\"title\"].tolist()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 16))\n",
    "\n",
    "wordcloud_2024 = WordCloud(background_color='white', colormap='tab20', width=800, height=600).generate(\" \".join(titles_2024))\n",
    "axs[0].imshow(wordcloud_2024, interpolation='bilinear')\n",
    "axs[0].axis('off')\n",
    "axs[0].set_title('KDD 2024 Word Cloud', fontsize=14)\n",
    "\n",
    "wordcloud_2025 = WordCloud(background_color='white', colormap='tab20', width=800, height=600).generate(\" \".join(titles_2025))\n",
    "axs[1].imshow(wordcloud_2025, interpolation='bilinear')\n",
    "axs[1].axis('off')\n",
    "axs[1].set_title('KDD 2025 Word Cloud', fontsize=14)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8ff6af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "[t for t in titles_2025 if \"graph\" in t.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30fe8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# 1) Graphを含むタイトル抽出（大小無視）\n",
    "df = pd.concat([df_2024[['title']], df_2025[['title']]], ignore_index=True)\n",
    "graph_df = df[df['title'].str.contains(r'\\bgraph(s)?\\b', case=False, na=False)].copy()\n",
    "titles = graph_df['title'].tolist()\n",
    "\n",
    "# 2) ルールベース分類（すぐ全体感を掴む）\n",
    "rules = {\n",
    "    \"GNN/Graph Neural Network\": r\"\\b(gnn|graph neural|message passing)\\b\",\n",
    "    \"Knowledge Graph / KG\": r\"\\b(knowledge graph|kg\\b)\\b\",\n",
    "    \"Heterogeneous / Attributed Graph\": r\"\\b(heterogeneous|attributed|multi-?relational|meta-?path)\\b\",\n",
    "    \"Temporal / Dynamic / Spatio-temporal\": r\"\\b(temporal|dynamic|time[- ]series|spatio|trajectory)\\b\",\n",
    "    \"Recommender on Graph\": r\"\\b(recommend|recommender|ctr|ranking)\\b\",\n",
    "    \"Anomaly / Fraud / Detection\": r\"\\b(anomaly|fraud|outlier|intrusion|detection)\\b\",\n",
    "    \"Generation / Diffusion on Graph\": r\"\\b(generate|generation|diffusion|synthesis)\\b\",\n",
    "    \"Contrastive / Self-supervised\": r\"\\b(contrastive|self[- ]supervised|ssl)\\b\",\n",
    "    \"Causal / Reasoning / Logic\": r\"\\b(causal|reasoning|logic|symbolic)\\b\",\n",
    "    \"LLM × Graph / KG-RAG\": r\"\\b(llm|large language|retrieval|rag|agent)\\b\",\n",
    "}\n",
    "\n",
    "def tag_title(t):\n",
    "    hits = [k for k, pat in rules.items() if re.search(pat, t.lower())]\n",
    "    return hits if hits else [\"Other\"]\n",
    "\n",
    "graph_df[\"rule_tags\"] = graph_df[\"title\"].apply(tag_title)\n",
    "\n",
    "# 集計\n",
    "rule_counts = (\n",
    "    graph_df.explode(\"rule_tags\")\n",
    "            .groupby(\"rule_tags\").size()\n",
    "            .sort_values(ascending=False)\n",
    ")\n",
    "print(\"=== Rule-based counts ===\")\n",
    "print(rule_counts)\n",
    "\n",
    "# 3) 自動トピック（NMF）: 「graph」等をストップワードに入れて本質語を浮かせる\n",
    "stop = set([\n",
    "    \"graph\",\"graphs\",\"based\",\"model\",\"models\",\"learning\",\"data\",\"via\",\"toward\",\"using\",\n",
    "    \"method\",\"framework\",\"approach\",\"task\",\"large\",\"scale\",\"neural\",\"network\",\"networks\"\n",
    "])\n",
    "vectorizer = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    ngram_range=(1,2),\n",
    "    max_df=0.6, min_df=2,\n",
    "    stop_words=stop\n",
    ")\n",
    "X = vectorizer.fit_transform(titles)\n",
    "\n",
    "n_topics = 8  # データ量を見て調整\n",
    "nmf = NMF(n_components=n_topics, random_state=0, init=\"nndsvd\").fit(X)\n",
    "W = nmf.transform(X)\n",
    "H = nmf.components_\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "def top_terms(component, k=10):\n",
    "    idx = component.argsort()[::-1][:k]\n",
    "    return [terms[i] for i in idx]\n",
    "\n",
    "print(\"\\n=== NMF topics (top terms) ===\")\n",
    "topic_terms = []\n",
    "for t_id, comp in enumerate(H):\n",
    "    words = top_terms(comp, k=10)\n",
    "    topic_terms.append(words)\n",
    "    print(f\"Topic {t_id}: {', '.join(words)}\")\n",
    "\n",
    "# 各トピックの代表タイトル（スコア最大上位3件）\n",
    "print(\"\\n=== Representative titles per topic ===\")\n",
    "for t_id in range(n_topics):\n",
    "    idx = W[:, t_id].argsort()[::-1][:3]\n",
    "    reps = [titles[i] for i in idx]\n",
    "    print(f\"\\n[Topic {t_id}]\")\n",
    "    for r in reps:\n",
    "        print(\" -\", r)\n",
    "\n",
    "# 4) KMeansでクラスタ名を自動案（上位語から命名ヘルパ）\n",
    "k = 8\n",
    "km = KMeans(n_clusters=k, n_init=\"auto\", random_state=0).fit(X)\n",
    "graph_df[\"kmeans_cluster\"] = km.labels_\n",
    "\n",
    "# 各クラスタの上位語\n",
    "import numpy as np\n",
    "print(\"\\n=== KMeans clusters (top terms) ===\")\n",
    "for c in range(k):\n",
    "    mask = (graph_df[\"kmeans_cluster\"] == c).values\n",
    "    centroid = km.cluster_centers_[c]\n",
    "    idx = np.argsort(centroid)[::-1][:10]\n",
    "    print(f\"Cluster {c}: {', '.join(terms[i] for i in idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c315d7bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# 1) Graphを含むタイトルだけ抽出\n",
    "def extract_graph_titles(df):\n",
    "    g = df[df[\"title\"].str.contains(r\"\\bgraph(s)?\\b\", case=False, na=False)].copy()\n",
    "    g[\"clean_title\"] = g[\"title\"].str.replace(r\"\\s+\", \" \", regex=True).str.strip()\n",
    "    return g\n",
    "\n",
    "g24 = extract_graph_titles(df_2024)\n",
    "g25 = extract_graph_titles(df_2025)\n",
    "all_graph = pd.concat([g24.assign(year=2024), g25.assign(year=2025)], ignore_index=True)\n",
    "\n",
    "# 2) ストップワード（listにするのがポイント）\n",
    "custom = {\n",
    "    \"graph\",\"graphs\",\"based\",\"model\",\"models\",\"learning\",\"data\",\"via\",\"toward\",\n",
    "    \"using\",\"method\",\"framework\",\"approach\",\"task\",\"large\",\"scale\",\n",
    "    \"neural\",\"network\",\"networks\"\n",
    "}\n",
    "stop_words = list(ENGLISH_STOP_WORDS.union(custom))\n",
    "\n",
    "# 3) TF-IDF（bi-gramで表現を拾う、少数データでも動くよう min_df=1）\n",
    "vec = TfidfVectorizer(\n",
    "    lowercase=True,\n",
    "    ngram_range=(1,2),\n",
    "    min_df=1, max_df=0.6,\n",
    "    stop_words=stop_words,\n",
    "    token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z\\-]+\\b\"  # meta-path, time-series なども拾う\n",
    ")\n",
    "\n",
    "X = vec.fit_transform(all_graph[\"clean_title\"])\n",
    "terms = vec.get_feature_names_out()\n",
    "\n",
    "# 4) NMFでトピック抽出\n",
    "n_topics = 8  # データ量で調整（多ければ増やす）\n",
    "nmf = NMF(n_components=n_topics, init=\"nndsvd\", random_state=0)\n",
    "W = nmf.fit_transform(X)\n",
    "H = nmf.components_\n",
    "\n",
    "def top_terms(component, k=10):\n",
    "    idx = component.argsort()[::-1][:k]\n",
    "    return [terms[i] for i in idx]\n",
    "\n",
    "print(\"=== NMF topics (top terms) ===\")\n",
    "for t_id, comp in enumerate(H):\n",
    "    print(f\"Topic {t_id}: {', '.join(top_terms(comp))}\")\n",
    "\n",
    "# 5) 年別のトピック強度（平均スコア）を見る\n",
    "all_graph[\"topic\"] = W.argmax(axis=1)\n",
    "topic_strength_by_year = (\n",
    "    pd.DataFrame(W).assign(year=all_graph[\"year\"].values)\n",
    "    .groupby(\"year\").mean()\n",
    ")\n",
    "print(\"\\n=== Topic strength by year (mean W) ===\")\n",
    "print(topic_strength_by_year)\n",
    "\n",
    "# 6) 代表タイトルを確認\n",
    "for t_id in range(n_topics):\n",
    "    idx = np.argsort(W[:, t_id])[::-1][:3]\n",
    "    print(f\"\\n[Topic {t_id} representatives]\")\n",
    "    for i in idx:\n",
    "        print(\"-\", all_graph.iloc[i][\"title\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83a9a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def make_ngram_wordcloud(titles, ngram_range=(2, 3), stop_words=\"english\", colormap=\"tab20\"):\n",
    "    # n-gram の頻度を集計\n",
    "    vectorizer = CountVectorizer(ngram_range=ngram_range, stop_words=stop_words)\n",
    "    X = vectorizer.fit_transform(titles)\n",
    "    freqs = dict(zip(vectorizer.get_feature_names_out(), X.toarray().sum(axis=0)))\n",
    "    \n",
    "    # WordCloud生成\n",
    "    wc = WordCloud(\n",
    "        background_color=\"white\",\n",
    "        colormap=colormap,\n",
    "        width=800,\n",
    "        height=600\n",
    "    ).generate_from_frequencies(freqs)\n",
    "    return wc\n",
    "\n",
    "titles_2024 = df_2024[\"title\"].tolist()\n",
    "titles_2025 = df_2025[\"title\"].tolist()\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(20, 16))\n",
    "\n",
    "# 2024年\n",
    "wc_2024 = make_ngram_wordcloud(titles_2024, ngram_range=(2, 2))\n",
    "axs[0].imshow(wc_2024, interpolation=\"bilinear\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[0].set_title(\"KDD 2024 Word Cloud (2 gram)\", fontsize=14)\n",
    "\n",
    "# 2025年\n",
    "wc_2025 = make_ngram_wordcloud(titles_2025, ngram_range=(2, 2))\n",
    "axs[1].imshow(wc_2025, interpolation=\"bilinear\")\n",
    "axs[1].axis(\"off\")\n",
    "axs[1].set_title(\"KDD 2025 Word Cloud (2 gram)\", fontsize=14)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30e5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "# ===== パラメータ =====\n",
    "NGRAM_RANGE = (2, 2)          # 2-gram以上\n",
    "TOPN = 50                     # 上位いくつを表示\n",
    "MIN_2025_COUNT = 3            # 2025で最低何回以上出たn-gramを対象にするか（ノイズ除去）\n",
    "EXTRA_STOP = {\n",
    "    # つなぎ語など無意味フレーズを追加で除外（必要に応じて増やしてください）\n",
    "    \"based on\",\"using\",\"with\",\"without\",\"by\",\"for\",\"of\",\"and\",\"to\",\"from\",\n",
    "    \"toward\",\"via\",\"state of\",\"real world\",\"large scale\",\"case study\"\n",
    "}\n",
    "STOP_WORDS = ENGLISH_STOP_WORDS.union(set(\" \".join(EXTRA_STOP).split()))  # 単語ベース除外\n",
    "\n",
    "# ===== 1) 一貫した語彙でベクトル化（結合してfit、各年にtransform） =====\n",
    "corpus_all = titles_2024 + titles_2025\n",
    "vectorizer = CountVectorizer(\n",
    "    ngram_range=NGRAM_RANGE,\n",
    "    stop_words=\"english\",     # 単語ストップは英語ベースで\n",
    "    lowercase=True,\n",
    "    min_df=1, max_df=0.9,     # 片寄り防止に高頻度語を少し落とす\n",
    "    token_pattern=r\"(?u)\\b[a-zA-Z][a-zA-Z\\-]+\\b\"\n",
    ")\n",
    "X_all = vectorizer.fit_transform(corpus_all)\n",
    "vocab = np.array(vectorizer.get_feature_names_out())\n",
    "\n",
    "# 追加のフレーズ除去（EXTRA_STOP はフレーズ単位）\n",
    "mask_keep = np.ones_like(vocab, dtype=bool)\n",
    "if EXTRA_STOP:\n",
    "    bad = np.array([any(bad_phrase == v for bad_phrase in EXTRA_STOP) for v in vocab])\n",
    "    # 「〜 of」「using the」など部分一致で落としたければ .contains に変更\n",
    "    mask_keep &= ~bad\n",
    "\n",
    "# 各年のカウント行列\n",
    "X_24 = vectorizer.transform(titles_2024)\n",
    "X_25 = vectorizer.transform(titles_2025)\n",
    "\n",
    "c24 = np.asarray(X_24.sum(axis=0)).ravel()\n",
    "c25 = np.asarray(X_25.sum(axis=0)).ravel()\n",
    "\n",
    "# ストップ対象を除外\n",
    "c24 = c24[mask_keep]\n",
    "c25 = c25[mask_keep]\n",
    "vocab = vocab[mask_keep]\n",
    "\n",
    "# ===== 2) 指標計算 =====\n",
    "n24 = len(titles_2024)\n",
    "n25 = len(titles_2025)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"ngram\": vocab,\n",
    "    \"count_2024\": c24,\n",
    "    \"count_2025\": c25,\n",
    "})\n",
    "# コーパス規模差を補正した頻度（100タイトルあたり）\n",
    "df[\"freq_2024_per100\"] = df[\"count_2024\"] / max(n24, 1) * 100\n",
    "df[\"freq_2025_per100\"] = df[\"count_2025\"] / max(n25, 1) * 100\n",
    "df[\"delta_abs\"] = df[\"count_2025\"] - df[\"count_2024\"]\n",
    "df[\"delta_per100\"] = df[\"freq_2025_per100\"] - df[\"freq_2024_per100\"]\n",
    "\n",
    "# スムージング付きの成長率（出現ゼロ対策）\n",
    "alpha = 0.5\n",
    "df[\"growth_ratio\"] = (df[\"count_2025\"] + alpha) / (df[\"count_2024\"] + alpha)\n",
    "df[\"is_new_in_2025\"] = (df[\"count_2024\"] == 0) & (df[\"count_2025\"] > 0)\n",
    "\n",
    "# ノイズ削減（2025で最低出現回数）\n",
    "df_filt = df[df[\"count_2025\"] >= MIN_2025_COUNT].copy()\n",
    "\n",
    "# ===== 3) ランキング出力 =====\n",
    "# ①純増（規模補正後）のランキング\n",
    "rank_delta = (df_filt\n",
    "              .sort_values([\"delta_per100\",\"count_2025\"], ascending=False)\n",
    "              .head(TOPN))\n",
    "\n",
    "# ②成長率（ある程度出現しているものに限定）\n",
    "rank_growth = (df_filt[df_filt[\"count_2024\"] >= 2]     # 極小母数を除外\n",
    "               .sort_values([\"growth_ratio\",\"count_2025\"], ascending=False)\n",
    "               .head(TOPN))\n",
    "\n",
    "print(\"\\n=== 2024→2025 増加量ランキング（per 100 titles 基準） ===\")\n",
    "print(rank_delta[[\"ngram\",\"count_2024\",\"count_2025\",\"freq_2024_per100\",\"freq_2025_per100\",\"delta_per100\"]]\n",
    "      .reset_index(drop=True))\n",
    "\n",
    "print(\"\\n=== 2024→2025 成長率ランキング（母数>=2 & スムージング） ===\")\n",
    "print(rank_growth[[\"ngram\",\"count_2024\",\"count_2025\",\"growth_ratio\"]]\n",
    "      .reset_index(drop=True))\n",
    "\n",
    "# 必要ならCSVに保存\n",
    "# rank_delta.to_csv(\"ngram_delta_rank.csv\", index=False)\n",
    "# rank_growth.to_csv(\"ngram_growth_rank.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9a9a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_titles_for_phrase(df, phrase):\n",
    "    \"\"\"\n",
    "    phrase: \"anomaly detection\" のような2-gram文字列\n",
    "    - ハイフン/空白を許容（例: anomaly-detection）\n",
    "    - 複数形を軽く許容（models など）\n",
    "    \"\"\"\n",
    "    w1, w2 = phrase.split()\n",
    "    # 後半語が model → models のように複数形の s を許容\n",
    "    plural_ok = r\"(s)?\" if w2.endswith(\"model\") or w2.endswith(\"Model\") else \"\"\n",
    "    pat = rf\"(?i)\\b{re.escape(w1)}[-\\s]+{re.escape(w2)}{plural_ok}\\b\"\n",
    "    m = df[\"title\"].str.contains(pat, regex=True, na=False)\n",
    "    return df.loc[m, [\"title\"]].copy()\n",
    "\n",
    "# --- 年別抽出 ---\n",
    "anom_2024 = extract_titles_for_phrase(df_2024, \"anomaly detection\")\n",
    "anom_2025 = extract_titles_for_phrase(df_2025, \"anomaly detection\")\n",
    "\n",
    "\n",
    "print(\"======== Anomaly Detection 2024:\", len(anom_2024))\n",
    "anom_2024 = anom_2024.reset_index(drop=True)\n",
    "for i, row in anom_2024.iterrows():\n",
    "    print(f\"{i+1}: {row['title']}\")\n",
    "print(\"======== Anomaly Detection 2025:\", len(anom_2025))\n",
    "anom_2025 = anom_2025.reset_index(drop=True)\n",
    "for i, row in anom_2025.iterrows():\n",
    "    print(f\"{i+1}: {row['title']}\")\n",
    "\n",
    "# 必要なら年情報を付けて結合＆保存\n",
    "anom_2024[\"year\"] = 2024\n",
    "anom_2025[\"year\"] = 2025\n",
    "\n",
    "anom_all = pd.concat([anom_2024, anom_2025], ignore_index=True)\n",
    "\n",
    "# 表示（Jupyterなら）\n",
    "# display(anom_all)\n",
    "# display(lm_all)\n",
    "\n",
    "# CSV保存したい場合\n",
    "# anom_all.to_csv(\"titles_anomaly_detection_2024_2025.csv\", index=False)\n",
    "# lm_all.to_csv(\"titles_language_models_2024_2025.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcdf194",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
